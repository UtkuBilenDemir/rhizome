---
title: Session_20231024
tags:
  - project_note 2023
  - uni
  - powi
---
|     Created      |  Last Modified   |       Exists Since        |
|:----------------:|:----------------:|:----------------:|
| `= this.file.ctime` | `= this.file.mtime` | `= date(now) - this.file.ctime`|

# Session_20231024

## Devlin et al. "Bert: Pre-training of deep bidirectional transformers for language understanding."

- pre-trained ny 11.038 books
- 30.000 token vocabulary
- Transformer Model
- Key distinction: bi-directional

BERT outperforms GPT models
-> But BERT is not great at generating text
-> Better suited for multilingual purposes
-> It captutes more context
## References
1. [[136041-1 SE Capabilities and Limitations of language-based AI models]]