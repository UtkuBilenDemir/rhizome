---
title: Transformer
tags:
  - project_note 2025
  - uni
  - powi
---
|     Created      |  Last Modified   |       Exists Since        |
|:----------------:|:----------------:|:----------------:|
| `= this.file.ctime` | `= this.file.mtime` | `= date(now) - this.file.ctime`|

# Transformer
> ing, the next anything, becomes a broader logic of ordering the world. The transformer breakthrough with “attention is all you need” parallelizes these sequences so that non-linear or distant dependencies can be made to matter. It is a speculative and predictive political order that promises to govern non-linear problems by attending to the “global dependencies” in all available unlabelled data. Finally, with the pre-training and fine-tuning paradigm, the desire for “zero shot” learning (classifying the previously unencountered) becomes simultaneously desired as a zero-shot politics of responding to all unencountered events. With the rise of the prompt, the generative model offers itself as an open-ended and experimental mode of governing, with each prompt seeking to elicit desired behaviours from model and from the world. [@amoore2024]

# References
1. [[Chapters]]