\chapter{AI}

\epigraph{
	[...] descending into the hidden abode of production means something else in the digital age. It means that we must also descend into the somewhat immaterial technology of modern-day computing, and examine the formal qualities of the machines that constitute the factory loom and industrial Colossus of our age. The factory was modernity’s site of production. The “non-place” of Empire refuses such an easy localization. For Empire, we must descend instead into the distributed networks, the programming languages, the computer protocols, and other digital technologies that have transformed twenty-first-century production into a vital mass of immaterial flows and instantaneoustransactions. Indeed, we must read the never ending stream of computer code as we read any text (the former having yet to achieve recognition as a “natural language”), decoding its structure of control as we would a film or novel.
	\parencite[82]{galloway2001}
}


The current chapter provides the technical foundation for the subsequent political analysis of generative AI. To properly understand the institutional role of \gls{genai} models, it is necessary to first trace their historical development, underlying architectures, and operational logics. While this section remains on a primarily technical level, it also gestures towards the epistemological and political stakes that will be elaborated later.

The chapter proceeds chronologically and conceptually. It begins by outlining the historical trajectory of artificial intelligence research, distinguishing between the early, symbolic paradigm (\gls{symai}) and the contemporary, statistical approaches that characterize deep learning and generative models. This includes an explanation of neural networks, self-supervised learning, and the rise of transformer architectures as the technical backbone of modern \glspl{llm}.

Beyond mere description, this technical overview serves a strategic purpose: it demonstrates how AI, even at the level of architecture, already embeds specific logics of inference, representation, and control. These are not neutral technical details, but the material conditions that enable AI systems to operate as infrastructures of knowledge production, decision-making, and ultimately, governance.

The subsequent sections therefore provide both the necessary technical background and the conceptual scaffolding for the analysis of generative AI as a distributed, non-symbolic institution of power that follows.

\section{(\hlred{A second intro to AI to introduce the problematic})}
\begin{orangebox}
	TBD
\end{orangebox}




\marginnote{
	\begin{orangebox}
		This part can be a good addition to the "state of art"

		\begin{itemize}
			\item \cite{montanari2025} is a good source for a brief techno-political
			      history and genealogy of llms
			\item Also here
			      https://www.technologyreview.com/2024/07/10/1094475/what-is-artificial-intelligence-ai-definitive-guide/
			      and here \cite{pasquinelli2023}
		\end{itemize}
	\end{orangebox}
}

\section{From Symbolic Rules to Statistical Modulation: A Brief History of \gls{ai}  and \gls{nlp} }

\Gls{nlp} is an area that lies at the intersection of linguistics, computer science, and AI, aiming to create computational systems that can interpret and handle human language data. it is not surprise that some of the most fundamental approaches in terms of \gls{ai} was achieved in \gls{nlp}.

Artificial intelligence emerged in the mid-20\textsuperscript{th} century, grounded in the formal logics of symbolic representation. The foundational paradigm, now referred to as \gls{symai} or \gls{gofai} , conceived intelligence as a matter of symbolic reasoning over explicitly encoded rules. Early AI systems aimed to emulate human problem-solving by manipulating structured propositions within formal languages. The assumption was clear: if the world could be faithfully translated into a logical schema, machines could infer, deduce, and act rationally (see \cite[183]{eloff2021}).


This early paradigm treated intelligence as a computational process operating over discrete symbols according to explicitly programmed rules. AI systems under this logic were built to emulate deductive reasoning and problem-solving: if the world could be encoded in a set of symbolic propositions, intelligent behavior could be generated by manipulating those propositions through logic \parencite[183]{eloff2021}. Relying on handcrafted rule sets, to recognise patterns the digit six in an image for instance, one might encode the features ``a closed loop at the bottom'' and ``a curve rising to the right.'' Such symbolic heuristics were sufficient so long as the data was clean and the context unambiguous. But real-world ambiguity proved hostile to symbolic systems. As \gls{symai} attempted to scale into more complex domains like vision or language, it revealed its brittleness \parencite[183--184]{eloff2021}. Philosophers of phenomenology were early critics of this paradigm. Hubert Dreyfus, among others, argued that human intelligence was not symbolic, but embodied, situated, and fundamentally non-representational \parencite{dreyfus2009}. Despite such critiques, \gls{symai} dominated the first decades of research. This rationalist framework aligned with early cognitive science’s attempts to model the mind as a rule-based machine of symbolic representation (see \cite[194--197]{montanari2025}).

The 1956 Dartmouth Conference institutionalized these ambitions by defining AI as ``the science and engineering of making intelligent machines'' \parencite[195]{montanari2025}. Systems like the Logic Theorist and expert systems in medicine or law exemplified this approach. Yet, these systems could not generalize beyond predefined rules. When confronted with noise or shifting contexts, their logic collapsed. The result was a period of stagnation and disillusionment now remembered as the ``AI Winters'' \parencite[183]{eloff2021}.

In \gls{ai}'s early years of development, Alan Turing made substantial contributions by introducing the famous "Turing Test" (or "the imitation game") that evaluates a machine's capability of imitating intelligence and rationality of a human along with the concept of a universal machine (see \cite[196]{montanari2025}). Although, as the chief scientist leading Meta's \gls{ai} development Yann LeCun notes that the Turing Test a bad test to evaluate any kind of \gls{ai} model \parencite[]{lexfridman2024}, Turing's contributions have played its role in the conceptualisation of a "prompt"-based "conversational machine" \parencite[196]{montanari2025}. This idea was mainly framed as if a machine could convince another human to being a human, it was conceived as intelligent. Onwards, the general purpose \gls{ai} development continued with ups and downs in activity, with a couple of earlier succesful neural network based aproaches like Mulloch-Pits, ELIZA program, up until around 1997 where much more advanced models like Deep Blue operating on more sophisticated architectures like \glspl{dnn} were developed \parencite[197]{montanari2025} .

As Deleuze also comments back, then, the hierarchically structured learning and
the projection of a central pattern
was not working well at the early stages:


\begin{quote}
	This is evident in current problems in information science and computer science, which still cling to the oldest modes of thought in that they grant all power to a memory or central organ. Pierre Rosenstiehl and Jean Petitot, in a fine article denouncing "the imagery of command trees" (centered systems or hierarchical structures), note that "accepting the primacy of hierarchical structures amounts to giving arborescent structures privileged status.... The arborescent form admits of topological explanation.... In a hierarchical system, an individual has only one active neighbor, his or her hierarchical superior.... The channels of transmission are preestablished: the arborescent system preexists the individual, who is integrated into it at an allotted place" (signifiance and subjectification). \cite[16]{deleuze1987}
\end{quote}


A turning point came in the 1990s with the rise of data-driven \gls{nlp}. As the internet boom suddenly introduced a massive digital corpora, researchers shifted toward statistical learning. This third era of NLP replaced hand-coded rules with empirical models trained on annotated examples \parencite{maas2023}. Models could now generalize from data rather than deduce from axioms. The real transformation, however, began in the early 2000s. Pushes through the ability to process more and more data allowed a new paradigm to emerge,  rooted in \glspl{nn} inspired by the architecture of the brain, \emph{connectionism}became the source of further advancements. These systems, now often termed \glspl{dnn}, learned not by logic but by adjusting distributed weightings across layered networks. This became the foundation for contemporary \gls{ml} and \gls{dl} systems. Exponential advances in computation enabled these networks to scale \parencite[184]{eloff2021}.

The arrival of self-supervised learning marked a milestone. Unlike supervised models, which require labeled data, self-supervised models learn by predicting missing elements from within the input itself; typically a masked or next word. This method allowed models to learn linguistic regularities from massive unlabeled corpora, and it gave rise to pre-trained \glspl{genai} \parencite{maas2023}. The architecture that enabled this leap was the transformer architecture. Its core mechanism, self-attention, computes weighted dependencies between all tokens in a sequence, allowing the model to capture long-range relations independent of word order. This innovation enabled massive parallelization and scalability \parencite{maas2023}.

A subcategory of the \gls{genai}, \glspl{llm} such as GPT-3 and GPT-4 are not task-specific in the traditional sense. Rather than being fine-tuned for each use case, they rely on ``few-shot prompting'': given a small set of examples at inference time, they condition their outputs without internal weight updates. Their knowledge is distributed across billions, sometimes trillions, of parameters trained to minimize prediction error. The shift from \gls{symai} to deep, generative architectures does not merely mark a technical transition. It signals a deeper epistemological break. \Glspl{llm} do not ``understand'' language in any classical sense, they generate statistically likely continuations. Meaning is no longer rule-based; it is computed as vector proximity in high-dimensional space \parencite[199]{montanari2025}. These networks are opaque, their training data culturally saturated, and their outputs probabilistic \parencite[186]{eloff2021}. They do not interpret, they modulate. Rather than representing knowledge, they operationalize its prediction. In doing so, they establish a new infrastructure for language: distributed, non-symbolic, and non-transparent.

This transformation, from formal logic to differential modulation, sets the stage for understanding \gls{genai} not just as a technical system but as an institutional form---a mechanism of governance, sense-making, and subjectivation in contemporary control societies.

%\section{From Rule-Based NLP to Generative Modeling}
%
%A historical typology of \gls{nlp} distinguishes four successive eras \parencite{maas2023}. The first (1950–1969) centered on word-level translation using rudimentary rule-based systems, constrained by limited linguistic and computational knowledge. The second (1970–1992) introduced more refined, hand-crafted rule systems that distinguished between declarative grammar rules and procedural language use. In the third era (1993–2012), the availability of digital text enabled supervised machine learning approaches to NLP, using annotated corpora to train statistical models that could generalize from labeled examples.
%
%\sidenote{\textbf{TODO:} Introduce better citations}
%
%The fourth era (2013–present) introduced deep learning and self-supervised training. This shift marked a decisive move away from symbolic encoding toward data-driven pattern recognition. \glspl{llm} learn by predicting masked or subsequent words in vast unlabeled text corpora. These self-supervised tasks enable the model to internalize complex linguistic regularities without human annotation, forming layered, distributed representations across billions of parameters \parencite{maas2023}.
%
%The Transformer architecture—now foundational to \glspl{llm}—relies on the mechanism of \gls{selfattention}, which computes weighted dependencies between all tokens in a sequence, allowing the model to integrate long-range context independently of position. This design enables massive parallelization and has proven essential to scaling up model complexity and generalization capabilities \parencite{maas2023}. Where early \glspl{ann} required \gls{finetuning} for specific tasks, modern \glspl{llm} operate through \gls{prompting}: conditioned on examples at inference time, they adapt behavior without internal parameter updates.
%
%This transformation marks a fundamental departure from rule-based reasoning to statistical sequence modeling. The model no longer ``understands'' language in a semantic sense but estimates the most likely continuation based on learned distributional patterns. \glspl{llm} thus operate as probabilistic engines of language generation—trained not to reason symbolically but to modulate plausible outputs within vast representational spaces \parencite{maas2023}.

%\section{Rise of Non-Symbolic AI and the Return of Connectionism}
%
%This mode of operation included the explicit definition of features to be interpreted. For example, to identify the number six, one might encode rules such as ``a closed loop at the bottom'' and ``a rising curve to the right.'' These heuristics were treated as sufficient for inference, provided the environment was clean, structured, and semantically unambiguous. Such assumptions persist in popular imaginaries, where intelligence is imagined as a disembodied capacity for symbolic manipulation. However, \gls{symai} struggled with tasks outside rule-bound domains and failed to process real-world complexity \parencite[183--184]{eloff2021}. Phenomenological critiques argued that intelligence is embodied, situated, and non-representational, challenging the epistemology of symbolic computation \parencite[183]{eloff2021}. As \textcite{montanari2025} shows, \gls{symai} remained an artifact of rationalist heritage, aiming to formalise human thought as rule-bound behaviour in tree-like hierarchies.
%
%Despite such critiques, \gls{symai} dominated early AI. The Dartmouth Conference in 1956, often cited as AI's point of origin, defined the field as the ``science and engineering of making intelligent machines,'' placing symbolic manipulation at the center of its epistemic frame \parencite[195]{montanari2025}. Early systems like Newell and Simon's Logic Theorist and expert systems in the 1970s operationalized this vision. But symbolic systems proved brittle in the face of ambiguity and noise. These limitations catalyzed the ``AI Winters'' of the 1970s and 1980s \parencite[183]{eloff2021}.
%
%The early 2000s marked the resurgence of \gls{ai} via connectionism. This paradigm, inspired by biological neural networks, abandoned symbolic representations in favor of statistical learning. Connectionist systems learn not by encoding logic but by optimizing distributed weights across layered artificial neurons. This turn was enabled by exponential growth in computational resources and data availability \parencite[184]{eloff2021}.
%
%Modern machine learning, particularly in deep learning systems, replaces symbolic reasoning with correlation. Input data is propagated through multiple hidden layers and transformed via activation functions. Training adjusts internal weights through \gls{backpropagation} and \gls{gradientdescent}, generating predictions that minimize statistical error rather than representational mismatch.
%
%This shift is not merely technical but epistemological. The internal layers of a \gls{dnn} do not correspond to interpretable features but encode a ``dramatization'' of statistical differentials \parencite[186]{eloff2021}. Pattern recognition is governed by statistical proximity, not meaning; learning is not semantic but topological and differential.
%
%\textcite{montanari2025} describes this shift as a ``renaissance of distributionalism,'' where meaning is no longer tied to syntactic rules but computed as vector proximity in high-dimensional space. Models such as Word2Vec, GloVe, BERT, and GPT encode semantics via contextual patterning. This enables inference, generation, and classification by mapping tokens across distributed embedding spaces \parencite[199]{montanari2025}.
%
%The implications are far-reaching. Deep learning architectures now underlie not only \glspl{llm} but also \glspl{gan}, recommendation engines, and real-time translation systems. These architectures enact new modes of subjectivation: they classify, filter, predict, and generate—without explaining. Their opacity, termed the ``black box problem,'' has given rise to the field of \gls{explainableai}, especially in response to bias and injustice in training data \parencite[186]{eloff2021}.
%
%Thus, contemporary \gls{ai} no longer aims to simulate reasoning but to model correlations. It is a statistical, opaque infrastructure—a topology of differential learning. If \gls{symai} sought to represent the world through logic, modern \gls{nonsymai} approximates it through modulation. This sets the stage for the next chapter: generative architectures as institutions of epistemic control.
%\hline
%
%A historical typology of Natural Language Processing (NLP) distinguishes four successive eras \parencite{maas2023}. The first (1950–1969) centered on word-level translation using rudimentary rule-based systems, constrained by limited linguistic and computational knowledge. The second (1970–1992) introduced more refined, hand-crafted rule systems that distinguished between declarative grammar rules and procedural language use. In the third era (1993–2012), the availability of digital text enabled supervised machine learning approaches to NLP, using annotated corpora to train statistical models that could generalize from labeled examples.
%
%The fourth era (2013–present) introduced deep learning and self-supervised training. This shift marked a decisive move away from symbolic encoding toward data-driven pattern recognition. Large Language Models (LLMs) learn by predicting masked or subsequent words in vast unlabeled text corpora. These self-supervised tasks enable the model to internalize complex linguistic regularities without human annotation, forming layered, distributed representations across billions of parameters \parencite{maas2023}.
%
%The Transformer architecture—now foundational to LLMs—relies on the mechanism of self-attention, which computes weighted dependencies between all tokens in a sequence, allowing the model to integrate long-range context independently of position. This design enables massive parallelization and has proven essential to scaling up model complexity and generalization capabilities \parencite{maas2023}. Where early neural networks required fine-tuning for specific tasks, modern LLMs operate through few-shot prompting: conditioned on examples at inference time, they adapt behavior without internal parameter updates.
%
%This shift in architecture and training method marks a departure from rule-governed systems to statistical sequence modeling. The model no longer “understands” language in a semantic sense but estimates the most likely continuation based on learned distributional patterns. In this new configuration, LLMs function as probabilistic engines of language generation—trained not to reason symbolically but to modulate plausible outputs within vast representational spaces \parencite{maas2023}.
%\section{Rise of the non-symbolic AI and the return of connectivism}
%
%Artificial intelligence emerged as a research domain in the mid-20th century, rooted in the formal logics of symbolic representation. This early paradigm, retrospectively labelled \gls{gofai} or \gls{symai}  , treated intelligence as a computational process operating over discrete symbols according to explicitly programmed rules. AI systems under this logic were built to emulate deductive reasoning and problem-solving: if the world could be encoded in a set of symbolic propositions, intelligent behavior could be generated by manipulating those propositions through logic (see \cite[183]{eloff2021}).
%
%This mode of operation included the explicit definitions of the features to be
%interpreted, for example, in the case of hand written number recognition to identify the number six, one might encode rules such as ``a closed loop at the bottom'' and ``a rising curve to the right.'' These symbolic heuristics were treated as sufficient for inference, provided the environment was clean, structured, and semantically unambiguous. Such assumptions persist in popular understandings of AI, where intelligence is imagined as a disembodied capacity for logic and symbolic manipulation . However, symbolic AI struggled with anything beyond the environments where rules and the goals can be explicitly defined and has never been remotely capable of processing real-world complexity (see \cite[183-184]{eloff2021}). Philosophers of phenomenology \sidenote{For example \cite{dreyfus2009}.}  challenged the underlying epistemology of \gls{symai}  early on, drawing on phenomenological aspects to argue that human intelligence is irreducibly embodied, situated, and non-representational \parencite[183]{eloff2021}. \Gls{symai} stayed as one of the remnants of the rationalist heritage of early cognitive science, aiming to formalise human thought as rule-bound behaviour  \parencite[194--197]{montanari2025} and tree-like hierarchical operation.
%
%
%Despite these critiques, symbolic AI dominated the early decades of the field. The Dartmouth Conference in 1956, widely regarded as AI's founding moment, introduced intelligence as the ``science and engineering of making intelligent machines,'' laying the groundwork for symbolic manipulation as AI’s epistemic foundation \parencite[195]{montanari2025}. Systems such as Newell and Simon's Logic Theorist and later expert systems in the 1970s were emblematic of this formal, logic-driven approach. However, symbolic systems proved fragile in the face of noisy, context-rich input and failed to generalize effectively. These limitations contributed to the ``AI Winters'' of the 1970s and 1980s—periods of reduced funding and diminished confidence in the field \parencite[183]{eloff2021}.
%
%The early 2000s marked the resurgence of AI via an alternative path: connectionism. This paradigm, inspired by biological neural networks, eschewed symbolic representations in favor of statistical learning. Connectionist systems learn not by encoding logic, but by optimizing distributed weightings across layers of artificial neurons. This transition was made possible by exponential increases in computational power and data availability, which allowed deeper architectures to emerge and scale \parencite[184]{eloff2021}.
%
%Modern machine learning, particularly in its deep learning form, replaces symbolic reasoning with correlation. In an artificial neural network (ANN), input data is transformed through multiple interstitial layers into output predictions via nonlinear activation functions. In training, systems such as deep convolutional networks adjust their internal weightings by minimizing error through backpropagation and gradient descent—procedures that resemble a differential field of local adjustments rather than a global model of truth.
%
%As Eloff emphasizes, this is not just a change in method but in epistemology. The internal layers of an ANN do not resemble human-understandable features but are ``dramatisations'' of intensive differences that function without semantic transparency \parencite[186]{eloff2021}. Recognition is not governed by meaning but by statistical proximity, and optimization does not follow interpretation but numerical minimization.
%
%Montanari traces this statistical turn to a broader ``renaissance of distributionalism,'' where meaning is computed via vector space proximity, not logic. Language models like Word2Vec, GloVe, BERT, and GPT operationalize this shift by learning relationships between tokens in high-dimensional space, enabling generation and inference based on contextual patterning rather than syntactic rules \parencite[199]{montanari2025}.
%
%The implications are vast. Deep learning systems now underlie generative adversarial networks (GANs), language models, recommendation systems, and more. These architectures do not merely automate tasks; they enact new modes of subjectivation. As Eloff notes, their inner workings are opaque, their outputs probabilistic, and their training datasets culturally saturated. While they may outperform symbolic systems on tasks like image or speech recognition, they are not explainable in classical terms. This opacity, termed the ``black box problem,'' has generated calls for explainable AI, especially in light of social bias and algorithmic injustice \parencite[186]{eloff2021}.
%
%Thus, contemporary AI no longer resembles the ideal of a rational decision-maker. It is a distributed, statistical, opaque infrastructure—an algorithmic topology of differential learning. Where symbolic AI aimed to represent the world, connectionist AI aims to approximate it—less an episteme than a diagram, less a logic than a modulation. This shift sets the stage for the next chapter: the institutional implications of generative architectures.
%
%
%
%
%\hline
%\section{\acrfull{symai}}
%
%\marginnote{
%
%	\begin{orangebox}
%		This part is where you explain the whole ordeal with the BA. Old fashioned AI,
%		how it related, what it achieved and lacked
%	\end{orangebox}
%}
%
%%WARNING:MAC
%Artificial intelligence today functions as a silent infrastructure within systems of algorithmic control, embedded across platforms that shape how we search, stream, invest, and even predict criminal activity. Public imaginaries often depict AI as a kind of elevated intellect, a rational mind crafted from intricate code and formal logic, capable of solving problems through internal representations of the world. Within this vision, intelligence appears as a matter of deduction: to identify the number six, one would encode a closed bottom loop and a rising curve, operationalising recognition as the execution of symbolic (albeit self-formed by the model) rules.
%%TODO: Use your example form mathematics?
%This logic-based approach, once dominant under the banner of ‘Good Old-Fashioned AI’ (GOFAI), still echoes in certain technical domains. However, its brittleness in the face of real-world ambiguity—where data is messy, contexts shift, and meaning resists neat classification—has long rendered it inadequate. Philosophers like Hubert Dreyfus challenged this model early on, arguing that intelligence cannot be confined to syntactic manipulation, especially when embodied perception and lived experience play central roles. As such critiques mounted and expectations deflated, AI research passed through periods of dormancy (known as “AI winters”) where optimism collapsed under the weight of its own abstraction (see \cite[183]{eloff2021}).


%\section{Non-symbolic AI}
%
%Since the early 2000s, AI has undergone a dramatic revival—not as a sudden reappearance, but as a reintensification of a trajectory that never fully dissipated. At the core of this renewed momentum lies the exponential growth in computational capacity, which has reinvigorated a paradigm once sidelined: connectionism. This approach, inspired by the structure and functioning of biological neural systems, had attracted early attention in AI’s formative years. However, its practical application was curtailed by the hardware constraints of the time. What was once set aside due to technological insufficiency has now resurfaced, empowered by the capacities of modern parallel processing and data availability (see \cite[183-184]{eloff2021}). This re-initialised course was the catalysor of the development of \glspl{ann}.
%%TODO: TUrn to @eloff2021 for ANN definition
%
%
%
%
%
%\epigraph{A computer would deserve to be called intelligent if it could deceive a human into believing that it was human.}{\textcite{turing1950}}
%
%Although it is mainly discussed in a linguistic and cognitive context, \gls{ai} connects itself to some broader themes \parencite[194]{montanari2025}. The term \gls{ai} was first coined in the Dartmouth Conference as an interdisciplinary field organized by John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon at Dartmouth College; this as McCarthy stated "the science and engineering of making intelligent machines" beginning with the early 1950s was entangled with the development of field cognitive science \parencite[195-196]{montanari2025}.
%
%In \gls{AI}s early years of development, Alan Turing made substantial contributions by introducing the famous "Turing Test" (or "the imitation game") that evaluates a machine's capability of imitating intelligence and rationality of a human along with the concept of a universal machine (see \cite[196]{montanari2025}). Although, as the chief scientist leading Meta's \gls{ai} development Yann LeCun notes that the Turing Test a bad test to evaluate any kind of \gls{ai} model \parencite[]{lexfridman2024}, Turing's contributions have played its role in the conceptualisation of a "prompt"-based "conversational machine" \parencite[196]{montanari2025}. This idea was mainly framed as if a machine could convince another human to being a human, it was conceived as intelligent. Onwards, the general purpose \gls{ai} development continued with ups and downs in activity, with a couple of earlier succesful neural network based aproaches like Mulloch-Pits, ELIZA program, up until around 1997 where much more advanced models like Deep Blue operating on more sophisticated architectures like \glspl{dnn} were developed \parencite[197]{montanari2025} .

\section{The Transformer Architecture: Infrastructure of Modulation}

\begin{orangebox}
	TBD, a general case of the development of transformers (e.g. \cite{vaswani2017a})
\end{orangebox}



\subsection{From Recurrent Bottlenecks to Attention-Based Modulation}
\label{subsec:transformer_modulation}

The Transformer architecture emerged as a break from the sequential bottlenecks of earlier neural models such as \glspl{rnn} and \glspl{cnn} . Both of these relied on \textbf{locality}: \glspl{rnn} processed input tokens one at a time, with each step depending on the hidden state of the previous one. Convolutional networks, while parallelizable, were constrained by \gls{kernel} sizes and fixed receptive fields. As a result, both architectures struggled to model long-range dependencies effectively, especially in complex natural language tasks \parencite[1-2]{vaswani2017a}.


\begin{quote}
	Consider the case of Transformer models, which exemplify the interplay between metaphor and function. Transformers, a specialized type of neural network, simulate certain structures and functions of the human brain, excelling at processing sequential data such as words in a sentence or notes in a melody. The transformative innovation within Transformers is the “attention mechanism,” which enables the model to focus selectively on the most relevant parts of the input sequence. This mechanism is pivotal for discerning complex relationships and dependencies within data. By revolutionizing natural language processing (NLP), Transformers have driven significant advancements in AI applications. The term “head” in Transformers, for instance, refers to the multi-head attention mechanism, a key feature that captures diverse aspects of an input sequence simultaneously. This dual role of technical objects – functionally specific and mythically resonant – reveals their broader cultural impact. Technical metaphors, often catachrestic and hybridized, solidify not only the utility but also the mystique and credibility of AI systems \parencite[206]{montanari2025} .
\end{quote}



In contrast, the Transformer dispensed with recurrence altogether. Instead, it introduced \emph{self-attention} as the central mechanism for computing representations. Self-attention allows every token in a sequence to attend to every other token simultaneously, computing a weighted sum of contextually relevant elements regardless of their position \parencite[4]{vaswani2017a}. This design eliminates the need for stepwise memory and enables models to integrate global information in a single layer, with no distance penalty. The result is a structure that lends itself to massive parallelization and scalability, two features foundational to contemporary \glspl{llm}. Unlike \glspl{rnn} or \glspl{cnn}, Transformer Networks do not rely on recursive feedback or localized convolutional loops. They do not recycle the output of a unit back into itself over time, nor do they constrain operations to spatially bounded kernels. Instead, through the mechanism of self-attention, each \gls{token} in the input sequence is made immediately available to every other token, establishing a \emph{global field of relation} across the entire sequence. This architecture affords a form of synchronic awareness: the model encodes each element not in isolation or temporal sequence, but through its distributed relevance to all others. In contrast to the iterative, memory-laden structure of \glspl{rnn}, or the fixed, spatial hierarchies of \glspl{cnn}, the Transformer’s design embeds the presence of every other word within the representation of each word. This spectral interdependence, where tokens are mutually inscribed into one another, suggests a structure in which meaning is always already haunted by the rest of the utterance (see \cite[12]{maas2023}).


Technically, self-attention calculates relationships between \glspl{token} by projecting them into \emph{query}, \emph{key}, and \emph{value} vectors. These are used to compute attention weights through dot-product similarity and softmax normalization. Each token's final representation is thus a weighted blend of all other tokens, modulated by their contextual relevance. Through multiple stacked layers and attention heads, the Transformer builds increasingly abstract representations, capturing both syntactic structure and semantic context.

This shift is not merely technical. It marks a transformation in how \gls{ai} systems model the world. Rather than operating through sequential representation, the Transformer operates through a form of constant \emph{distributed modulation}. Transformers make it possible for every element on the network entangled



this architectural shift can be understood through the logic of \textbf{double articulation} (see \cite{ai-inquiry2025a}). The Transformer operates simultaneously on two strata: a molecular level of local attention scores and parameter updates, and a molar level of structured linguistic understanding. Each token’s representation is formed through dynamic micro-adjustments, distributed flows of relevance, not unlike the first articulation of matter into expressive form. This is then consolidated across layers into coherent linguistic function, the second articulation of those forms into stable semantic structures. The Transformer thus embodies the double articulation of machinic sense-making.

Attention mechanisms enact selective intensities across this field. Rather than representing fixed symbols, the Transformer’s architecture instantiates meaning as a function of weighted relationality. These differential proximities constitute a \emph{diagrammatic space}, where meaning emerges not from rules but from patterns of modulation. It is here that Deleuze and Guattari’s distinction between molar and molecular formations becomes productive: the Transformer is not a symbolic machine but a machinic assemblage that captures both distributed flows and structured outputs simultaneously (see \cite{ai-inquiry2025a}).

Attention weights instantiate selective intensities between elements, constituting a diagrammatic field of relations. In this field, meaning is not fixed or rule-governed; it is a function of differential proximity and relational salience. The Transformer thus encodes a new mode of learning: not inference from rules, but modulation of difference through weighted connection.

This transformation prepares the ground for the following sections, which analyze core Transformer mechanisms; attention, gradient descent, backpropagation, not merely as computational techniques but as micro-political operations that govern the production of meaning and subjectivity under algorithmic regimes.

% \subsection{Attention: Distributed Desire and Selective Intensities}

\subsection{Under- \& Overfitting}

\begin{orangebox}
	TBD
\end{orangebox}



\subsection{Gradient Descent: Sinking into the Manifold}

Gradient descent is a fundamental optimization algorithm used to train neural networks by iteratively updating model parameters in the direction that reduces the loss function. This process can be interpreted as a movement through the high-dimensional loss manifold, gradually approaching minima where the model performs optimally on a given task. In the architecture of \gls{dl} , gradient descent operates not merely as a tool of optimisation, but as a process of traversal across a manifold shaped by error surfaces and loss functions. Each step taken by the model through its parameter space is a micromovement within this multidimensional topography, adjusting internal configurations in relation to perceived error, or deviation from the desired output. This movement is neither deterministic nor purely reactive; it is a dynamic rearticulation of relations within the network, guided by the flow of gradients.


Formally, for a differentiable loss function \( L(\theta) \), the update rule is:
\[
	\theta_{t+1} = \theta_t - \eta \nabla L(\theta_t)
\]
where \( \theta \) represents model parameters, \( \eta \) is the learning rate, and \( \nabla L(\theta_t) \) is the gradient of the loss function with respect to the parameters at iteration \( t \) \cite{tarmoun2024}.

In the context of transformer-based models, particularly those employing attention mechanisms, the dynamics of gradient descent reveal unique challenges. A critical issue arises from the Softmax function used in attention layers. The Jacobian of the Softmax function induces a form of preconditioning, which can severely distort the curvature of the loss landscape, especially when attention distributions are sparse. This leads to \textit{ill-conditioning}, where the convergence of gradient descent is slowed due to steep or flat directions in parameter space, resulting in inefficient optimization.

Recent theoretical analyses show that:
\begin{itemize}
	\item In \textbf{overparameterized settings}, where the number of model parameters exceeds the number of training examples, gradient descent can still converge linearly under smoothness and Polyak-Łojasiewicz (PŁ) conditions.
	\item In \textbf{realistic, underparameterized settings}, however, gradient descent struggles to converge due to the highly variable conditioning introduced by Softmax Jacobians \parencite[8-9]{tarmoun2024}.
\end{itemize}

Gradient descent is a function that minimises the error between predictions by
adjusting the weight of the stronger options. It is a way for neural network to
reach towards the better answer instead of getting stuck in similarly good
answers whenever the number of possible candidates for an predictions are high.
It is a way of emphasising small distinctions into bigger ones until one of the
options stand out. And in a visual sense, this is finding the local minimum of
a manifold.

To illustrate how gradient descent works in practice, consider a model trying to distinguish between handwritten digits, such as "6" and "8". At the beginning of training, the model's predictions are almost random. After seeing one example of a "6" misclassified as an "8", the algorithm computes how much each parameter (e.g., a weight in the network) contributed to the error. Gradient descent then updates these parameters slightly in the direction that would have made the prediction more accurate. This process repeats for many examples, gradually adjusting the model to reduce its overall error. The model is slowly emphasising through the repetitions (\glspl{epoch}) what made different examples most distinct, and exaggerating those differences.


Rather than a simple algorithmic mechanism, gradient descent can be interpreted as an expression of difference-in-repetition in the Deleuzian sense: each pass through the data does not reproduce identical results but modulates the model’s internal structure through iterative exposure. The model does not approach a universal form but develops an operational sensitivity to local singularities distributed within the training data. In this sense the gradient descent's contribution to model's learning from a
dataset resembles Deleuze's analysis of difference in repetition \parencite{deleuze1994}. The model finds itself in a vast amount of repetition through \glspl{epoch} with subtle adjustments in each step barely recognisable, whereas the differences get slowly established and/or more emphasised. Through these subtle differences and adaptations on the nodes, emerging patterns make it possible for model to recognise further patterns. The model is not starting from a presupposed \textit{model} but drives the \textit{model} through the interaction with the data
\sidenote{However not to forget that this learning is completely bound to the scope of data. An LLM for example is purely encircled in the language it has been exposed to.}.
A trained model that appears to “know” an image of a tree, for instance, has not encoded a definition, but has undergone enough transformations to resonate with distributed features constituting “treeness” across the dataset. This is not epistemology in the classical representational sense, but a diagrammatic form of learning: one that forms through modulation and intensity rather than classification and identity. Gradient descent, in this framework, appears not as descent toward a pre-defined minimum, but as an ongoing negotiation across a surface of potentials, a diagrammatic inscription of learning as continuous variation.


%INFO:Activate thiese
%\begin{orangebox}
%	\textbf{Main Points}
%	\begin{itemize}
%		\item AI models undergo multiple passes over the same dataset during training; each complete cycle is referred to as an epoch.
%		\item With every epoch, the model’s internal parameters are incrementally adjusted to reduce prediction error.
%		\item This iterative refinement constitutes a form of “repetition that produces difference”; over time, the model develops the capacity to recognize increasingly complex patterns.
%	\end{itemize}
%\end{orangebox}
%
%
%\begin{orangebox}
%	\begin{itemize}
%		\item Deleuzian repetition: Epoch based repetition \parencite[]{ai-inquiry2025}.
%		\item Patterns recognised through repetition (seemingly ineffective in
%		      isolation)
%		\item Knowledge is obtained rhizomatically, because there is no overarching
%		      meaning imposed in it (it is unsupervised).
%		\item Arguably, the algorithm is nothing but pure Schizz operating on a
%		      corpus. Nothing but a productive core (desiring-production?)
%		\item Each neuron is a machine but meaningless on its own.
%	\end{itemize}
%\end{orangebox}


\subsection{Back Propagation}
In early forms of symbolic artificial intelligence, often referred to as \gls{gofai}, the process of inference followed a rigid \textit{forward propagation} model. Logical rules, handcrafted by programmers, operated on symbolically encoded inputs to produce outputs through a chain of deductive reasoning steps. While this framework could simulate intelligent behavior in constrained environments, it lacked scalability and adaptability. The system could not revise its internal structure based on errors or feedback; any misclassification required manual rule modification.

The limitations of GOFAI became increasingly apparent in tasks involving ambiguity, noise, or vast data spaces, domains where human cognition thrives not by rule-following but by plastic, adaptive learning. To address this shortcoming, neural network researchers introduced \textit{backpropagation} as a general algorithmic solution that allows networks to \emph{learn} from error. Rather than only pushing activations forward, as in GOFAI, backpropagation pushes \emph{errors backward} through the network to update internal parameters and improve future predictions.

Backpropagation thus constitutes a bidirectional mechanism: during the \textit{forward pass}, inputs are transformed into outputs through successive layers; during the \textit{backward pass}, the discrepancy between the prediction and the target is used to adjust the weights in a way that gradually minimizes this error.

Formally, the weight update rule in backpropagation is given by:
\[
	w^{\text{new}} = w^{\text{old}} - \eta \frac{\partial E}{\partial w}
\]
where \( \eta \) is the learning rate and \( \frac{\partial E}{\partial w} \) is the partial derivative of the error function \( E \) with respect to the weight \( w \) \parencite{hecht-nielsen1992}. This formulation ensures that each parameter is updated in proportion to how much it contributed to the error.

\textcite{hecht-nielsen1992} describes backpropagation as a paradigm-shifting method for approximating functions \( f: \mathbb{R}^n \to \mathbb{R}^m \) using layered neural structures. Unlike Hebbian learning, which depends on co-activation, backpropagation relies on the explicit transmission of error signals. These signals traverse the network in reverse order, enabling a distributed form of learning where each parameter is tuned with respect to its role in the total output error.

While, backpropagation reconfigures the architecture of learning itself: not as a static application of encoded knowledge, but as a dynamic modulation of internal configurations in response to external feedback, it also gears the system to be extremely feedback oriented.


\begin{orangebox}
	Considering these differences, rather than directly equating AI learning with "desiring-machines," it becomes important to consider how AI produces and manages "desire" within
	%capitalist 
	systems. For example, recommendation systems and targeted advertising AI play roles in stimulating, directing, or transforming human desires. From this perspective, AI might be functioning more as a "device for managing desire" rather than as "desiring-machines."

	In more emergent approaches like self-supervised learning and generative models, there's a tendency to emphasize internal exploration over explicit external goals. These could be considered partially approaching the non-teleological aspects of "desiring-machines," but they still cannot be understood separately from social and economic contexts. \cite{ai-inquiry2025}
\end{orangebox}





\section{Undistributed}
\begin{orangebox}
	The following are partly random notes
\end{orangebox}



%\textbf{gradient descent}
%Learning, then, becomes an immanent process of becoming-with the data. As the algorithm moves through layers of abstraction, adjusting weights via the backpropagation of error signals, it constructs a statistical responsiveness to patterns, not by identifying essence, but by modulating intensities.
%
%
%But this descent is not only numerical. Conceptually, it evokes a dynamic of immersion; a topology of modulation where learning proceeds by navigating the manifold defined by statistical errors. In this sense, gradient descent marks the shift from symbolic abstraction toward embodied, emergent learning – not a plan imposed from above, but a felt orientation within the surface of mistakes. It is the system's way of modulating itself toward improved performance through internal responsiveness rather than external supervision.
%
%From a Deleuzian perspective, one could argue that gradient descent enacts the logic of immanence. Learning does not proceed by rule-following or representational correctness, but by intensive traversal; by reorienting the network within its own surface of potentialities. It is not the recognition of fixed categories that matters, but the generation of ever more efficient mappings between input and output spaces within the logic of the machine. Here, the error is not a failure but a productive force: a site of transformation.
%
%In this respect, gradient descent can be seen as the machinic actualization of a diagrammatic logic: it does not follow a single trajectory, but continuously folds and unfolds itself across the loss surface, modulating its pathways based on local curvature. The movement is continuous, sensitive, and decentralized – a process of fine-tuned adjustment grounded in the relational fabric of the network’s topology. It does not seek to control from above; rather, it sinks into the logic of the system, reshaping itself in contact with the terrain it traverses.

\section{AI as Desiring Machine}

Deleuze and Guattari’s reconceptualization of desire in \textit{Anti-Oedipus} disrupts its traditional framing as a lack or absence. Rather than being tethered to objects or driven by deficiency, desire is reframed as inherently constructive,a dynamic process that connects, produces, and transforms. This reconceptualization unfolds through the figure of the \textit{desiring-machine}: a machinic assemblage that links with other machines to process flows, cut them, and redirect them toward novel arrangements \parencite{deleuze1983}.

In this light, contemporary neural architectures resonate strikingly with the logic of desiring-machines. Each unit within a neural network, a node, a layer, acts as a site of transmission, where inputs are transformed into outputs through learned transformations. These local operations accumulate, forming an extended architecture wherein every connection carries the potential for reconfiguration. Far from being fixed, the network’s internal relations are perpetually reshaped through iterative exposure to data.

The training process becomes a clear instantiation of this machinic productivity. With each pass through a dataset, gradients modify internal parameters, not to install fixed representations but to increase the model’s responsiveness to patterns distributed across inputs. The model gradually develops an attunement to features that were previously imperceptible, adjusting the weight and significance of signals over time. Through this recursive adaptation, distinctions become magnified, and latent regularities emerge as active differentials in the system’s outputs.

This iterative modulation, a form of learning through micro-adjustments,closely mirrors Deleuze’s philosophical conception of difference as immanent to repetition \parencite{deleuze1994}. Neural networks do not seek to reproduce a stable identity but continually reshape their internal structure in response to variation. The output of a well-trained model is not a mirror of the data but a trajectory produced by interactions with distributed intensities across the training manifold.

Seen from this perspective, generative AI systems are not merely computational artefacts; they function as technopolitical agents embedded in broader ecologies. Their outputs (texts, images, decisions) are not isolated results but points of articulation in a much larger relay of flows that include users, institutions, infrastructures, and ideologies. The productivity of these systems is not limited to the generation of content; it also participates in shaping forms of subjectivity, regimes of truth, and new forms of desire. In that sense, the neural network is not just a machine that learns, but a machinic topology of desire, operating not to fulfill lack, but to propagate relations.

\begin{orangebox}
	\begin{itemize}
		%\item Desire, in Deleuze and Guattari’s terms, is a productive and connective force; it emerges through processes, not lacks.
		\item Neural networks operate through interconnected transformations that mirror the logic of desiring-machines.
		\item Training unfolds through repeated modulation, where difference accumulates and internal structures evolve.
		\item Generative AI systems inhabit and influence wider assemblages, modulating subjectivity and cultural production through their outputs.
		\item U: \gls{genai} models are essentially nothing but a productive core.
		      Looking only for connections and building flows.
	\end{itemize}
\end{orangebox}


%\section{Forward and Backward Propagation}
%\label{sec:propagation}
%
%Neural networks operate in two complementary phases: a feedforward pass that computes outputs from inputs, and a backward pass (backpropagation) that updates model parameters based on error. This dual process transforms stateless inference into adaptive learning.
%
%\subsection{Forward Propagation --- A One-Way Flow}
%\textbf{Technical definition (source):} In a feedforward neural network, the input data is passed through successive layers where each neuron computes a weighted sum of its inputs, adds a bias, and applies a non-linear activation. This process continues until the final output is produced.
%
%Conceptually, forward propagation resembles classical symbolic or GOFAI systems: the network processes input according to pre-defined weights (rules), producing a deterministic result. No adjustment occurs during this step, it merely evaluates.
%
%\medskip
%
%\subsection{Backward Propagation --- The Learning Channel}
%\textbf{Technical definition (source):} Backpropagation efficiently computes gradients of a loss function with respect to each weight by recursively applying the chain rule from output layers back to input layers. It reuses intermediate computations, avoiding repetition.
%
%\medskip
%
%\textbf{Narrative integration:}
%\begin{itemize}
%	\item Forward propagation resembles a one-way inference engine, running fixed pathways through a network.
%	\item Backpropagation, by contrast, propagates \emph{error signals} backward, modulating weights based on how much they contributed to incorrect outputs.
%	\item Together, forward and backward passes establish a closed-loop learning system: the forward pass proposes, the backward pass adjusts.
%\end{itemize}
%
%\medskip
%
%\subsection{Why Backpropagation Was Needed}
%Before backpropagation, performing credit assignment across hidden layers was intractable. The algorithm enables deep architectures to tune hundreds of millions of parameters jointly, transforming them into adaptive systems rather than static classifiers.
%
%\medskip
%
%\subsection{Philosophical Reading: Micro-flows of Adaptation}
%Forward propagation alone is akin to a static logic, rule-based, one-directional. Backpropagation, however, introduces a feedback loop of \emph{difference-in-repetition}: each epoch subtly shifts the network’s internal structure, tuning it to data. Over training cycles, this back-and-forth flow embeds an emergent “sense” of patterns, not through symbolic encoding, but through the accumulation of micro-adjustments that reconfigure the network.

\subsection{From Pre-Training to Fine-Tuning: Modulating the Model's World}

Contemporary \glspl{llm} are trained through a bifurcated process: pre-training followed by fine-tuning. This division is more than procedural, it indexes a shift in epistemological orientation, from general pattern discovery to context-sensitive modulation. As \textcite[964]{dishon2024} notes, these two phases form the backbone of \gls{genai} development and its increasingly situated capabilities.

During pre-training, the model is exposed to enormous corpora of unlabeled text. This phase is governed by self-supervised learning, where the model predicts masked or subsequent tokens within sequences, gradually building statistical representations of language, syntax, and world-knowledge. Pre-training does not assume fixed semantic targets. Instead, it generates vast, opaque vector spaces structured by correlation, not comprehension. The model's representational capacity emerges not through symbolic grounding but through distributional regularity, a probabilistic resonance of patterns over linguistic terrain.

Fine-tuning operates differently. Here, the pretrained model is constrained and directed, often via Reinforcement Learning from Human Feedback (RLHF). This phase involves targeted adjustments to align the model's outputs with human-defined norms, tasks, or values. The goal is not to re-train from scratch, but to selectively amplify certain behaviors and suppress others, effectively sculpting the model’s general capacities into usable forms. As \textcite[964]{dishon2024} emphasizes, fine-tuning introduces a more deliberate epistemic framing, transforming the model into a more predictable and legible actor within specific sociotechnical domains.

This trajectory, from expansive, indeterminate modeling to focused, value-laden calibration, marks a shift in the way meaning is operationalized. In pre-training, the model becomes a medium for representing statistical potentials; in fine-tuning, it is molded into an instrument of specific sense-making. The move from probabilistic openness to contextual closure reflects a diagrammatic logic of control: generative architectures first deterritorialize meaning through scale, then reterritorialize it through prompt design, safety layers, and alignment regimes.

\medskip

%\subsection{Section Summary}
%\begin{orangebox}
%	\textbf{Key takeaways:} \begin{itemize}
%		\item Forward propagation performs input-to-output inference via fixed parameter paths (feedforward).
%		\item Backpropagation transmits the loss-derived ‘error’ in reverse, updating those parameters.
%		\item Together they form a feedback-based optimization loop: propose (forward), adjust (backward), repeat, enabling networks to learn from data.
%	\end{itemize}
%\end{orangebox}
