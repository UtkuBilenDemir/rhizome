\chapter{Agency – Latency – World Model: GenAI as Institution}

\section{The World Model: A Neoplatonic Representation}
In the discussion of the technical machinery of the \gls{genai} models,
especially with the obsevation of the \glspl{llm} in Chapter \ref{cha:ai}
, we have observed the models' tendency to build an overarching distribution of
the given data. This is to say, the gravitational pull of the main distribution
the model extracts from the data is always lingering on the meaning generated
with the given input at any time. In cognitive science and AI areas, this is often described in terms of a
\emph{world model}: a compact internal structure that allows an agent
to anticipate and navigate situations beyond its direct experience. Although there is a rich history of theory
about how humans perceive the world, and certainly not far away from Deleuze's
core works like \citetitle{deleuze1994}, there is considerable amount of
representational theory pointing out the mental model of the world humans
produce to process the \textit{external}. As the computer scientist
\citeauthor{forrester1971} \parencite*{forrester1971}  notes, humans do not imagine the entire
world in detail, but rely on \emph{selected concepts and their
	relationships} to operate effectively.


Similarly, \textcite{lecun2022} argues that autonomous intelligence
requires a \emph{configurable world model} capable of
generalizing, simulating, and guiding actions in unfamiliar contexts
rather than merely reacting to inputs. In the context of
\gls{genai} models, we often focus on how they parse training data
into meaningful outputs, yet for \gls{ai} research this
representational fabric carries a broader significance. A central
challenge is precisely how such models can \emph{generalize to interact
	with the world and solve problems they have never encountered before}
\parencite{lecun2022}; a question that remains pivotal for robotics and,
more broadly, the pursuit of \gls{agi}.


\marginnote{\textbf{TODO}:
	\begin{todolist}
		\item Introduce neo-platonism
		\item Introduce the critique of \cite{amoore2024}
		\item Potentially also \cite{eloff2021}
		\item This is also relevant to \cite{bender2021b}

	\end{todolist}
}



But, what does it mean for machines to possess representations of the reality? In earlier paradigms of \gls{ai}, the connection between data and meaning was structured through a \gls{sl} framework: models were trained to assign labels to inputs based on human-defined categories. This approach enacted a \textit{discriminative} logic, in which decision-making was organized around predefined classes and expected outputs. While the \glspl{nn} were still building their own unique patterns to solve the problems the they were projecting a pre-assessed human interpretation on the problems.

However, particularly following the participatory turn of the internet, as the volume and heterogeneity of data exploded, this model quickly revealed its limitations (see Chapter \ref{cha:ai} for a detailed analysis).

The need to extract structure from unlabeled data catalyzed a shift toward \gls{ul} techniques which immediately leads to \gls{genai} models finding their patterns uninstructed in the data . In \gls{nlp}, these approaches aimed to capture the statistical regularities of language without requiring explicit annotation. As \textcite[3]{amoore2024} notes, this shift also marks the emergence of a new political logic, one embedded not in symbolic rules or normative standards but in the infrastructures of estimation. \gls{genai} models no longer rely on explicit classification schemes; instead, they operate by sampling from high-dimensional distributions learned across vast corpora. Decisions and outputs no longer stem from deterministic reasoning, but from probabilistic approximations of \textit{underlying joint distributions}. The  claim of the data having an underlying distribution underneath, waiting to be extracted, is an assumption, an assumption of the truth being hidden in any given collection of data waiting to be discovered by the model.

\sidenote{\textbf{NOTE}: The neoplatonic assumption (e.g. \cite{eloff2021}) is stemming from here. The
	assumption of the truth being already contained in the given content, it is
	just waiting to be \textit{mined}. Potentially also connects to the Foucault's
	claims about the neoliberal governmentality.}

This reconfiguration has implications for how political reasoning is encoded and enacted. Generative systems interpolate across massive, heterogeneous data spaces to produce coherent outputs that appear viable, even when no predefined category exists. In applied contexts, ranging from healthcare and border control to military logistics, fine-tuned models are not merely tools of decision support. Rather, they shape the very space in which decisions become intelligible. Instead of selecting from a fixed menu of options, these systems generate a field of possibility conditioned by prior distributions. This transformation heralds the rise of an epistemology of inference ; a mode of reasoning grounded not in deliberation or rule-based classification but in the traversal of probabilistic space (see \cite[4-6]{amoore2024}) . Within this paradigm, actions and decisions emerge as expressions of what the model can estimate and simulate as plausible. Decision-making becomes immanent to the model’s internal structure: an act of interpolation rather than reflection. This logic resonates with Foucault’s analysis of how statistical inference became the objects of modern governance \parencite[108--109]{foucault2009a}. Yet in the case of generative models, the shift is even more radical: not only are populations modeled and estimated, but the structure of political possibility itself becomes coextensive with the space of learned distributions. As \textcite[3-6]{amoore2024} explains, the \textit{pathologies of disclassification} no longer describe models that fail to fit reality into stable categories; rather, the categories themselves are internalised within the training data. Discrimination and bias are not errors at the margins, they are conditions embedded in the latent architecture of inference.

Meaning-making and decision-making within these models diverge sharply from traditional symbolic approaches. %Rather than being rule-bound or semantically interpretable, outputs emerge from the interplay of statistical regularities encoded in the data. 
This is not simple parroting, as critiques such as \textcite{bender2021b} have proposed; it is a process of reconstitution, where the past is reformulated as the ground for plausible futures. The generative model becomes a site of epistemic production: one that configures knowledge not as correspondence, but as coherence within a distributional regime. We are though, beyond bias or discrimative algorithms produced through labels and
toppling the previous critical literature on AI. \textit{The pathologies of
	disclassification} \parencite[3]{amoore2024} are over, not because the
discrimination or the bias is eliminated from the model, but the new axiom of
the model training is the labels, structures, distributions of truth are
already immanent in the data itself (see \cite[3]{amoore2024}), governing logic
is directly parsed from the given data substance. Meaning-making,
decision-making over the latent distributions are different than the parroting
(see e.g. \cite{bender2021b}), the models create an ambigious politics of
knowledge, they are not simply repetitions of a faulty pattern in the data,
they are the product of some probability distribution found as the ideal
substance by the model, the question is if there is a structure to it.
%TODO: Now, you need to take one more look at bender 2021b and tell if amoore
%is really correct about this one.


\begin{orangebox}
	\textbf{Consider the following (from (Undistributed))}

	From a classical sociological perspective,most notably that of Max Weber,modern Western societies are fundamentally shaped by processes of rationalization. Bureaucracy, in Weber’s formulation, becomes the quintessential mode of organizing social life through formalized procedures, calculability, and the pursuit of technical efficiency. It is the institutional embodiment of rational order, characterized by impersonal authority and rule-governed decision-making \parencite[46]{kivisto2013}.

	In the context of algorithmic infrastructures and AI systems, this rationalizing logic is not only extended but intensified. Decision-making is increasingly delegated to computational procedures whose operations exceed the perceptual and procedural boundaries of traditional institutions. These systems do not merely reflect bureaucratic order,they operationalize it at a new scale and speed, embedding rationality within architectures of code, data, and optimization. As such, automation emerges as a hyperrational stratum of governance, inheriting the logic of bureaucratic control while displacing its human intermediaries.
\end{orangebox}




\subsection{Latency}
\marginnote{\textbf{TODO}: Explain dimensionality reduction in the previous
	chapter.}


\marginnote{\textbf{TODO}:
	\begin{todolist}
		\item \cite{lecun2015} and \cite{lecun2022} seem to be good sources for the
		technicality of this

		\item A potential way to counter Amoore is in \cite[3]{beckmann2023}
		\begin{quote}
			NR takes cognitivism’s representationalism to its extreme by making a claim about our conscious experience: we experience a brain generated model of these entities (Churchland \& Sejnowski, 1990; Mrowca, et al., 2018; Sitzmann, et al., 2020); for criticism see (Zahavi, 2018; Hipólito, 2022). Frith puts it in slogan form: “my perception is not of the world, but of my brain’s model of the world” (Frith, 2007).
		\end{quote}


		\item Introduce a section called "Representation and Re-presentation" ?
		Refer to \cite{beckmann2023}, Sartre, imaging consciousness.

	\end{todolist}
}



The political and ethical stakes of this transformation lie in generative AI’s capacity to govern through latent structures. They do not enforce norms; they encode tendencies. They do not decide in the traditional sense; they make certain decisions more likely to emerge than others \sidenote{See e.g. how \textit{gradient descent} makes the model emphasize \textit{relatively stronger} distributions even if the difference was negligible at the beginning; similarly, how the \textit{back propagation} continuously update the whole network with \glspl{epoch} of repetition in Sections \ref{gradient_descent} and \ref{back_propagation}} . However, in order to make the data more managable, and the patterns more visible, the model applies a dimensionality reduction to the data. Dimensionality reduction is a foundational technique in machine learning, far predating the rise of \gls{genai}. It allows models to project high-dimensional data, such as raw image pixels or token embeddings, into a compressed latent space that is tractable for statistical operations. These latent representations are not merely a technical convenience; they are the terrain upon which inference, generalization, and generation occur.

In this process, each data object,be it a sentence, an image, or a behavioral trace,is mapped onto a point or trajectory within a lower-dimensional space. The resulting representations emphasize the most \emph{distinctive} features relevant to the dataset as a whole. As \textcite[4]{amoore2024} argues, this latent space becomes the epistemological substrate of generative systems: not a reflection of the world, but a reconfiguration of its informational residues into governable form.


\begin{quote}
	More often than not, hidden layers have fewer neurons than the input layer to
	force the network to learn compressed representations of the original input.
	For example, while our eyes obtain raw pixel values from our surroundings,
	our brain thinks in terms of edges and contours. This is because the hidden
	layers of biological neurons in our brain force us to come up with better
	representations for everything we perceive. \parencite{buduma2022}
\end{quote}

This transformation echoes a shift identified by \textcite[7--9]{foucault2012a} in the historical sciences: where discontinuity once marked a failure of historical narrative, it now becomes a method of epistemic individuation. Historians seek not seamless continuities but ruptures, thresholds, and points of inflection. Similarly, generative AI models do not aim to preserve continuity with the world but to extract probabilistic logics from its discontinuities. The latent space becomes a topology of plausible transformations,an infrastructure for projecting coherence from fragments.

This is not a neutral act of compression. As Amoore notes, the reduction into latent space implies a governance logic: what is preserved, amplified, or discarded in the compression process shapes what becomes visible and actionable. The model’s world is not a mirror of the real, but a field of decision possibilities constructed through statistical filtration. The distance between the input and its latent encoding is not merely dimensional, it is political. The process can be simplified as the model bringing the data itself into a more simpler form with "more holes", and then filling in the holes with the rationality already derived from the same data. these latent representations “forge probabilistic proximities between data points, enabling inferences to be made in the absence of direct evidence.” The latent space is thus a site where knowledge is not verified but inferred, where truth is no longer deduced but estimated. It is where the governable becomes manifest through the model’s trained perception of pattern and variation \parencite[5]{amoore2024}.

In this sense, generative AI enacts a shift from representation to modulation. Latency is not about hiding; it is about restructuring. What appears as compression is in fact an operation of reorganization,a mapping of the world into the model’s differential calculus. The model does not need to see the world as it is; it only needs to predict what it believes the world can become. This structural logic is not limited to language or vision. It underlies the architectures of recommendation systems, predictive policing, and personalized healthcare, where actions are taken not on the basis of direct evidence but on probabilistic interpolation. The latent space is thus a new political territory, one where governance proceeds not through law or classification but through inference and projection.

\begin{orangebox}
	Herein lies the double-bind of generative infrastructures: the speculative space of model output,what is likely, coherent, or novel,is always haunted by the empirical foundation on which the model was trained. The world is not represented, but rendered through compression, interpolation, and emergence. It is a world governed by the \textit{modelled real}, where the limits of possibility are not drawn from law or debate, but from the statistical borders of a distribution. The generative model thereby emerges not just as a computational artefact, but as a political actor,one whose authority lies in its capacity to make decisions appear immanent, natural, and unarguable.
\end{orangebox}


%\section{Kafka’s Trial and the Logic of Indeterminate Governance}
%
%\marginnote{\textbf{TODO}:
%	\begin{todolist}
%		\item Especially \cite{dishon2024}
%		\item But also the idea of discreetness and continuity maybe?
%
%	\end{todolist}
%}
%
%
%
%Among the most powerful metaphors for the governance logic of \gls{genai} systems is Franz Kafka’s \textit{The Trial}. In contrast to the anthropomorphic imaginary of AI as a discrete, external agent,an intelligent Frankenstein,the Kafkaesque metaphor foregrounds the opacity, recursion, and futility of interacting with a system whose logic is both impersonal and deeply personal. As Dishon argues, generative AI reflects a form of governance not by rational authority, but by procedural unknowability \parencite{dishon2024}.
%
%In Kafka’s court, judgment is inevitable, but its basis is undisclosed. The subject is implicated not by transgression, but by the mere fact of interaction. Likewise, in \gls{genai} systems, there is no fixed authority, no explicit norm or rule; instead, there is a continuous process of generation, correction, and alignment. The user engages a system that appears to understand, but whose internal logic remains inaccessible, uninterrogable, and recursive.
%
%This structure of interaction produces a form of control that is fundamentally indeterminate. The user is not disciplined but absorbed,drawn into a feedback loop that reshapes behavior and expectation without ever declaring its aims. Governance emerges not from law, but from alignment; not from command, but from modulation. The result is a subject who is governed through interaction, shaped by inference, and captured in the recursive play of probability and plausibility.
%
%In this respect, generative AI enacts a model of control that is deeply affective. It produces uncertainty not only in epistemic terms, but in ontological ones: Who is speaking? Who is responsible? What does it mean to be answered? These are not simply philosophical questions; they are structural features of the human-machine interface under conditions of algorithmic control. The Kafkaesque character of this relation lies in its circularity: the more one engages, the more deeply one is implicated.
%
%The image of Kafka’s court,endlessly procedural, devoid of resolution,thus serves as an allegory for the governance logic of \gls{genai}. It is not that the system hides its operations; it is that the operations are no longer susceptible to knowing. Modulation becomes its own justification, and the subject is left not with answers, but with statistically generated meanings, tailored to preferences that may no longer be their own.


%\section{2Kafka’s Trial and the Logic of Indeterminate Governance}
%
%The sociotechnical imaginary of artificial intelligence has long been haunted by the figure of Frankenstein’s monster. In this vision, machines are imagined as discrete agents that awaken into consciousness, assert agency, and ultimately rebel against their creators. As \textcite[966--968]{dishon2024} notes, this anthropomorphic metaphor saturates public fears about \gls{genai} models: they are often imagined as autonomous subjects that might cross a emotional, rational, moral threshold and exceed their intended boundaries. The core anxiety is one of lost control: that a human-like entity might turn against its makers with the same volition and cunning that define human agency.
%
%Yet this imaginary, misreads the contemporary dynamics of \gls{genai} (see e.g. \cite{dishon2024} or \cite{prinsloo2017}) . Rather than an external actor exercising agency over a passive world, today’s AI systems operate as recursive assemblages,systems whose effects are not exerted through domination, but through continuous alignment and feedback. To capture this shift, \cite{dishon2024} turns to a different literary figure: Franz Kafka’s \textit{The Trial}. The Kafkaesque imaginary does not dramatize rebellion or personhood; it stages the breakdown of transparency, the erosion of discrete subjectivity, and the recursive entanglement of individuals with procedural logics they cannot access or influence.
%
%In \textit{The Trial}, the protagonist Franz K. is arrested without explanation, judged by a court whose procedures are invisible, and implicated in a system that neither condemns nor exonerates him. When he finally encounters the courtroom, a judge informs him: “The court does not want anything from you. It accepts you when you come and it lets you go when you leave” \parencite[970--971]{dishon2024}. The court is not a Leviathan issuing decrees, but a decentralized process that absorbs Franz K.’s attention, energy, and interpretive labor without providing any determinate meaning. His guilt is neither substantiated nor refuted,it is ambient, situational, and always-already assumed. The logic of this court is procedural, not juridical; affective, not epistemic.
%
%\Gls{genai} models might be deploying the replication of the same process. They do not dictate outcomes; they generate frames of plausibility. They do not assert will; they infer likelihoods. As \textcite[971]{dishon2024} notes, interactions with such systems require constant maintenance and engagement, yet never resolve. They promise personalization but deliver inscrutability. They mirror the user, only to displace authorship into a machinic elsewhere. Like Kafka’s court, their operations are not secret, they are simply inaccessible to human deliberation. This is not governance by rules, but governance by recursion.
%
%The result is a form of control that blurs the boundary between human and machine agency. In contrast to the Frankensteinian fear of a singular machine uprising, the Kafkaesque dynamic is one of entanglement. Meaning emerges not from human intention or machine will, but from a recursive interplay of prompting, feedback, and statistical inference. \Gls{genai} systems are trained on human-produced corpora, fine-tuned by user interactions, and recursively reintegrated into new datasets. As \textcite[973--974]{dishon2024} observes, this process erodes the boundary between human and machinic authorship. The outputs appear novel, not because they are authored, but because they cannot be traced to any singular source. In their continuous implementation of meaning-making the models do not have a state where a flow gets broken, continuous communication means continuous stream of information generated by the model, and because of the nature of the production the information does not have to match any previous output.
%
%This recursive dynamic produces not autonomy, but ambiguity. The user is not dominated, not necessarily steered to a specific direction but absorbed. They are transformed from author to editor, from creator to curator of machine-suggested text. \Gls{genai} expands the expressive field while simultaneously narrowing its contours through probabilistic bias and normative coherence. As Dishon concludes, agency under generative AI is neither internal nor external, but diffused across a black-boxed system that structures what is writable, sayable, or thinkable \parencite[974--975]{dishon2024}. \textcite{prinsloo2017} also reads this metaphor by applying the Kafkaesque imaginary to algorithmic decision-making in education. He argues that students subjected to opaque systems of prediction and evaluation,systems they cannot query or challenge,experience a similar erosion of agency. It is not that they are punished; it is that they are processed. Judgment emerges, but its logic remains untouchable. The Kafkaesque machine governs not by law but by uncertainty, not by punishment but by preemption.

%In this light, \gls{genai} does not stand in opposition to humanity, as Frankenstein once did; it recodes humanity’s conditions of sense, judgment, and agency through recursive probabilistic inference. It is neither a monster nor a judge,it is the court itself, endlessly procedural, affectively absorptive, and structurally illegible.


\section{Agency; Kafka’s Trial and \textit{limitless postponements}\parencite[5]{deleuze1992a} }
\label{Agency}

The sociotechnological imaginary of artificial life is historically shaped by
anthropomorphic assumptions. \citeauthor{dishon2024} points this out through the
example of Frankenstein's Monster. What is being communicated through
Frankenstein's Monster is an entity taking a human form and starting to
develop a human-like mind that leads to human feelings, thoughts, and a very
human-like experience of existential crisis. The discrete presentation of
artificial life mirrors human agency, which immediately becomes
associated with the
fear of losing control over an entity seeking to exercise agency. In its
anthropomorphic form of operation, artificial life frees itself from an
inferior position to dominate its environment and other species around it (see
\cite[966]{dishon2024}).

The worries about \gls{genai} follow a similar course. Anthropomorphic
assumptions point to the risk of \glspl{gm} going beyond their boundaries and
acting outside their intended programming in a human-like desire for domination \parencite[967-968]{dishon2024}.
In this sociotechnological imaginary, one very similar to our own, the Frankensteinian logic
obscures the actual nature of current human-\gls{ai} interaction.
\cite{dishon2024}'s analogy to explore this is via Kafka's *The Trial*. This piece
of literature, often used to reflect on bureaucratic
structures in modern society (e.g. \cite{deleuze2008}), also serves as a powerful
analogy to analyse information systems in terms of technological development.
An increasing number of authors (see e.g.
\cite{prinsloo2017, dishon2024}) have used it to reflect on an
increasingly algorithmically governed world.

Kafka's protagonist Franz K. finds himself in custody without knowing anything
about his alleged crime. The police officers arresting him know nothing
about the accusations, or whether any charges exist at all. Franz K. is
unable to locate, let alone process, any rationale or reasoning
behind the court's actions. While Franz K.'s futile attempts to uncover a clue
continue, \cite{dishon2024} notes a remark made by the judge when Franz K. accidentally finds
the room where his court is being held: "The court does not want anything from you. It accepts you when you come and it lets you go when you leave."

In contrast to the anthropomorphic nature of the Frankenstein analogy,
\textit{The Trial} offers a distinctly alternative structure: the court is
not bound to any kind of understanding of \textit{truth}; it operates
independently and is based on the subjectivities of the accused (see
\cite[970]{dishon2024}). While the court does not deploy any agency itself, it
nonetheless enacts a profound blocking or blurring effect on any agency the accused
may have initially possessed. Any discrete piece of subjectivity becomes
blended into an unidentifiable mass through constant echoing and distortion
\parencite[970]{dishon2024}. Furthermore, the connection between the events
inside the court and those in the outside world is blurry at best. The entire process might be
framed within a penal code or related to Franz K.'s actions,
but it might just as well be a completely self-contained environment in which
nothing exists but the process itself \textit{reacting} to Franz K. on a
\textit{token-to-token} basis. The lack of identifiable agency continues alongside
the absence of any intelligible communication regarding the core operating principles of
the court. We learn that others have tried to influence the court’s decision-making
mechanism,asking about their court date or complaining about their suffering,to no avail; no one is able
to affect it in any intelligible way.

We also find out that complete acquittal is impossible, and an \textit{apparent
	acquittal} means that the accused remains under constant pressure and can be arrested at any time,even immediately after
being released \parencite[971]{dishon2024}. Paradoxically, this makes the best
strategy for dealing with the court ensuring that the process never ends: "Interactions with the court are necessary and require constant maintenance, yet they cannot be controlled, predicted, or even expected to progress towards a resolution" \parencite[971]{dishon2024}. The court depicts
a logic of control in meaning-making entities, shifting from a stable, general
(and algorithmic) mode of meaning to a personalised one (see \cite[971]{dishon2024}),one operating in a
modulating manner. It is both personally tailored and inaccessible. As Franz K.
tries to obtain a comprehensive picture of the whole structure, the reader is
also led to constantly build and rebuild a stable, coherent understanding
of the text,yet the semblance only signifies its inaccessibility \parencite[972]{dishon2024}.

This analogy leads to a different question about discreteness: is agency a binary condition, especially when it comes to interactions between humans and meaning-making entities? In the Kafkaesque imaginary, agency is not neatly divided into internal and external domains, nor does it rest on a clear boundary between machine and human intentionality. Rather, generative AI exemplifies a recursive and entangled sociotechnical assemblage in which meaning emerges through blurred and distributed processes. GenAI is not positioned as an external actor acting upon a passive human world; its so-called intelligence is trained on human-produced data, reflecting statistical regularities identified in large-scale corpora. Yet this is not mere mirroring; its outputs are shaped through black-boxed processes that generate new, partially unpredictable meanings. As these outputs are increasingly used and re-integrated into future training data, the distinction between human and machine authorship erodes. Researchers have shown how this recursive structure reinforces mutual adaptation: models are fine-tuned to reflect human preferences, even at the expense of accuracy; users, in turn, modify their interpretive and communicative strategies to better align with the affordances of the system. In this way, meaning production is no longer attributable to a singular locus of agency. GenAI generates outputs that appear novel not because they emerge from a conscious subjectivity, but because they cannot be traced back to any specific author,human or otherwise. This increasingly invites the attribution of authorship or agency to the model itself, even though the technology remains deeply embedded in human practices of use, fine-tuning, and interpretation. As such, agency in the age of generative AI resists dichotomies of internal and external; instead, it operates across a diffuse and recursive terrain, in which the epistemological ground of intentionality is rendered unstable.

As Franz K., in the absence of a definite answer, constantly
searches for the truth, he resembles the perpetual process of seeking and finding
meaning while there is no clear indication of truth or agency. While \gls{genai} has been criticised for reproducing biases in its training data, it is equally crucial to recognise that its generative design, combined with the human drive to interpret, does not simply reflect meaning but perpetually modifies it,producing layered, elusive structures of signification and meaning without necessarily coming closer to any truth (see \cite[973--974]{dishon2024}).

Although speculative narratives about super-intelligent AI dominate public discourse, the more immediate concern lies in how generative AI subtly restructures the dynamics of control, choice, and coercion.
GenAI generates personalised outputs tailored to individual users, yet these outputs are shaped by internal processes that remain largely inaccessible,thereby complicating the distinction between voluntary choice and algorithmic coercion. This interplay does not replace human agency but reconfigures it within a black-boxed system that generates meaning at scale while framing the horizon of what is writable, sayable, or thinkable. Rather than simply offering more options, GenAI floods the field with tailored outputs whose structure and logic are not user-determined, but only user-aligned,often subtly guiding users toward normative formats and interpretive templates. As such, GenAI shifts the role of the writer from creator to editor of machine-generated content, simultaneously expanding expressive capacity and constraining it within machinic grammars of probability and preference (see \cite[974--975]{dishon2024}).



%\section{Kafka’s Trial and the Logic of Indeterminate Governance}
%
%The sociotechnical imaginary of artificial intelligence has long been haunted by the figure of Frankenstein’s monster. In this vision, machines are imagined as discrete agents: artificial beings that awaken into consciousness, assert agency, and ultimately rebel against their creators. As \textcite[966--968]{dishon2024} observes, this anthropomorphic metaphor continues to saturate public fears about \gls{genai}: models are framed as autonomous entities that might cross emotional, rational, or moral thresholds, exceeding their intended boundaries. At the heart of this narrative lies an anxiety of lost control, the fear that an artificial subject will exercise volition and domination against its makers.
%
%Yet this imaginary misreads the actual dynamics of contemporary \gls{genai}. Rather than an external agent asserting dominance over a passive world, today’s AI systems operate as recursive sociotechnical assemblages, configurations whose effects unfold not through autonomy, but through continuous feedback, statistical inference, and opaque alignment \parencite{prinsloo2017, dishon2024}. To grasp this shift, \textcite{dishon2024} proposes a different literary figure: Franz Kafka’s \textit{The Trial}.
%
%In contrast to the Frankensteinian fantasy of autonomous rebellion, Kafka’s world stages the breakdown of transparency, the erosion of discrete subjectivity, and the recursive entanglement of individuals with procedural logics they cannot access or influence. In \textit{The Trial}, the protagonist Franz K. is arrested without explanation, judged by a court whose procedures remain invisible, and implicated in a system that neither condemns nor exonerates him. When Franz K. finally stumbles upon the courtroom, a judge informs him: “The court does not want anything from you. It accepts you when you come and it lets you go when you leave” \parencite[970--971]{dishon2024}. The court is not a sovereign issuing decrees, but a decentralized process that absorbs Franz K.’s attention, energy, and interpretive labor, without providing any determinate meaning. His guilt is neither substantiated nor refuted; it is ambient, situational, and always already assumed.
%
%Generative AI mirrors this recursive, absorptive structure. These systems do not dictate outcomes; they generate frames of plausibility. They do not assert will; they infer likelihoods. As \textcite[971]{dishon2024} emphasizes, interactions with such systems require constant maintenance and engagement, yet never "a determinate" resolve. They promise personalisation and adaptability but not necessarily deliver certainty. They mirror the user, only to displace authorship into a machinic elsewhere. Like Kafka’s court, their operations are not secret, they are structurally inaccessible to human deliberation. This is not governance by explicit rules; it is governance by recursion. The Kafkaesque model reframes how agency itself is understood. The boundary between human and machine agency no longer holds. \Gls{genai} systems are trained on vast human-produced corpora, fine-tuned through user interactions, and recursively reintegrated into new datasets. Meaning production thus becomes a distributed, entangled process. As \textcite[973--974]{dishon2024} and \textcite{prinsloo2017} argue, authorship erodes in this recursive circuit. Outputs appear novel, not because they are authored by a human or a machine, but because they emerge from an opaque interplay of prompting, feedback, and probabilistic inference. The result is not autonomy, but ambiguity for anything that isn't a direct information contained in the data as it is to be presented. "The user" is transformed from author to editor, from creator to curator of machine-suggested content. \Gls{genai} expands expressive fields while simultaneously narrowing their contours through statistical bias, normative coherence, and probabilistic filtering. Meaning emerges through continuous streams of generated information, without the stability of a clear authorial source or the rupture of discrete breaks in production.
%
%
%This analogy leads to a different question about discreteness: is agency a binary condition, especially when it comes to interactions between humans and meaning-making entities? In the Kafkaesque imaginary, agency is not neatly divided into internal and external domains, nor does it rest on a clear boundary between machine and human intentionality. Rather, generative AI exemplifies a recursive and entangled sociotechnical assemblage in which meaning emerges through blurred and distributed processes. GenAI is not positioned as an external actor acting upon a passive human world; its so-called intelligence is trained on human-produced data, reflecting statistical regularities identified in large-scale corpora. Yet this is not mere mirroring; its outputs are shaped through black-boxed processes that generate new, partially unpredictable meanings. As these outputs are increasingly used and re-integrated into future training data, the distinction between human and machine authorship erodes. Researchers have shown how this recursive structure reinforces mutual adaptation: models are fine-tuned to reflect human preferences, even at the expense of accuracy; users, in turn, modify their interpretive and communicative strategies to better align with the affordances of the system. In this way, meaning production is no longer attributable to a singular locus of agency. GenAI generates outputs that appear novel not because they emerge from a conscious subjectivity, but because they cannot be traced back to any specific author,human or otherwise. This increasingly invites the attribution of authorship or agency to the model itself, even though the technology remains deeply embedded in human practices of use, fine-tuning, and interpretation. As such, agency in the age of generative AI resists dichotomies of internal and external; instead, it operates across a diffuse and recursive terrain, in which the epistemological ground of intentionality is rendered unstable.
%
%As Franz K., in the absence of a definite answer, constantly
%searches for the truth, he resembles the perpetual process of seeking and finding
%meaning while there is no clear indication of truth or agency. While \gls{genai} has been criticised for reproducing biases in its training data, it is equally crucial to recognise that its generative design, combined with the human drive to interpret, does not simply reflect meaning but perpetually modifies it, producing layered, elusive structures of signification and meaning without necessarily coming closer to any truth (see \cite[973--974]{dishon2024}). By design, \gls{genai} is  on a  recursive loop of endless adaptation, the continous design is incapable of finding out there might be no answer to the question, even when there is no question. The continous nature of the communication lacks the initation of the necessary breaks in communicative flows to allow production/creativity to grow.

\section{Personalisation and Probabilistic Meaning-Making}

\begin{orangebox}
	Especially incomplete
\end{orangebox}



If modulation defines the mode of governance in control societies, personalization constitutes its most pervasive expression. Within \gls{genai} systems, personalization does not emerge as an added feature, but as a constitutive function. These models operate by internalizing patterns across massive corpora of language, behavior, and context, generating responses that are not merely grammatically plausible, but contextually aligned with user input and platform-specific expectations. The effect is one of intimate plausibility: the sense that the model “understands” or “responds” in a way that feels personally attuned, despite the absence of semantic intention.

This dynamic is enabled by the probabilistic architecture of transformer-based models. In systems such as \glspl{llm}, every output is the result of a sampling operation across a distribution of possible continuations. Meaning, in this context, is not derived from an external referent or symbolic logic, but from the statistical coherence of the model's internal representations. Personalization emerges through fine-tuning, reinforcement learning from human feedback (RLHF), and user interaction histories,techniques that further entrench a recursive, data-driven alignment between dividual subjectivities and machinic outputs.

\marginnote{\textbf{TODO}: Title
	\begin{todolist}
		\item Introduce RHLF citation \cite{bai2022}
		\item Introduce the critique of \cite{eloff2021}

	\end{todolist}
}



Yet, Amoore \parencite{amoore2024} argues that the personalization offered by \gls{genai} is not emancipatory. Rather, it encodes what Amoore \parencite{amoore2024} identifies as a shift toward algorithmic plausibility: a regime in which truth is replaced by coherence, and where verisimilitude displaces verification. These models do not strive to represent the world accurately; they aim to produce outputs that are locally acceptable within the distributional field they have learned. In doing so, they participate in what Deleuze and Guattari describe as the “production of reality” by machinic assemblages \parencite{deleuze1983}.

\textbf{This has profound implications for the production of subjectivity. Personalization in this sense does not merely tailor outputs; it reshapes the terrain of what appears possible, relevant, or thinkable. By reinforcing patterns and filtering deviation through layers of probabilistic modulation, \gls{genai} systems enact a form of soft coercion,a modulation of expectation rather than a violation of autonomy. The user is not told what to think, but gradually inducted into a space of statistically prefigured sense.}

In this way, \gls{genai} participates in the ongoing reconfiguration of subjectivity under contemporary capitalism. By continuously adjusting outputs to align with learned preferences and contextual patterns, it constructs dividual selves whose coherence is maintained through feedback and reinforcement, not identity or agency. This is not the personalization of individual difference, but of algorithmic similarity,a personalization that works by making the subject more compatible with the model.

\section{Enregistrement and Subjectivation}


\begin{orangebox}
	Especially incomplete
\end{orangebox}

Within Deleuze and Guattari's machinic ontology, \emph{enregistrement} refers to the process by which flows are inscribed, segmented, and organized within a system. It is the function that captures and fixes movement, enabling the emergence of structured forms from differential intensities (see \cite[4]{deleuze1983}) . In the context of \gls{genai}, enregistrement takes on a new institutional form: the large-scale inscription of language, behavior, and intention into model weights, training sets, and interface design.

Every interaction with a generative system is a moment of recording, not simply in the technical sense of data logging, but in the diagrammatic sense of encoding relations into a machinic structure. Prompts become signals, completions become training feedback, and user corrections feed into broader patterns of reward and weighting. The system does not merely respond; it accumulates, modulates, and reconfigures itself across successive interactions. In this sense, the model is not static infrastructure, but a dynamic surface of enregistrement,an institutional body without organs.

This process is not neutral. It constitutes a new mode of subjectivation: one in which the user becomes legible not as an individual agent, but as a series of statistical affordances. Subjectivity here is not represented but assembled. The “user” is parsed, fragmented, and reaggregated across vector spaces, embeddings, and attention weights. What emerges is a dividual subject: a machinically inferred bundle of preferences, linguistic habits, and response tendencies, optimized not for autonomy but for coherence within the model's distributional field.

This machinic subjectivation is infrastructural. It takes place not through coercion or symbolic interpellation, but through continuous modulation,an ongoing inscription of behavior into computational space. The subject becomes a site of governance by virtue of being inscribed, rendered actionable, and modulated in real time. As such, \gls{genai} systems must be understood not only as epistemic or technical instruments, but as institutional agents participating in the construction and circulation of contemporary subjectivity.

\subsection{Language and the Subordination to Voice}

\begin{orangebox}
	This is an experimental section that goes with the following argumentation:

	Derrida's logcentrism -> writing is inferior to speech, orphaned from speech (a
	latency) ->

	\begin{quote}
		Thus, Derrida deduces that language is characterized by a lack or absence: it can never completely capture reality to present it as present, but its structure is fundamentally dictated by the very absence of that reality. Consequently, the signs making up language are to be interpreted as supplements, but in a much deeper sense of the term supplement: they serve as an external addition, supplementing for an internal void. Meaning only emerges as an added component. However, this addition, the supplement, simultaneously obscures the meaning it just revealed. This playful elusiveness of meaning is probably best explained through Derrida’s principle of differance. \parencite[8]{maas2023}
	\end{quote}

	-> Deleuze and the subordination to the voice in despotic machine -> This is
	also a latency
\end{orangebox}



\subsection{Language in \Gls{llm} }
\begin{orangebox}
	Incomplete
\end{orangebox}


\marginnote{\textbf{TODO}:
	\begin{todolist}
		\item This is yet an experimental one, come back to flesh it out.

	\end{todolist}
}


\begin{quote}
	It represents nothing, but it produces. It means nothing, but it works.
	Desire makes its entry with the general collapse of the question "What does
	it mean?" No one has been able to pose the problem of language except to the
	extent that linguists and logicians have first eliminated meaning; and the
	greatest force of language was only discovered once a work was viewed as a
	machine, producing certain effects, amenable to a certain use.
	\cite[109]{deleuze1983}
\end{quote}


\begin{quote}
	To the extent that LLMs excel at conversation, they verify Saussure’s insight that meaning emerges from the interplay of signs in a formal system. There is no inherent need for actual sensory grounding. If “a sign stands in the place of something else” (Saussure, 1959, p. 66), then for an LLM, the “something else” could be another cluster of words, or a swirl of pixels if it is visually enabled, all existing within the confines of digital memory. Meanwhile, Peirce’s emphasis on iconic signs , signs that resemble their object , and indexical signs , signs that point to or are causally connected with their object , seems, on the surface, less relevant to an AI that navigates text tokens rather than the physical world. Without a body to roam or eyes to see, the Peircean structure appears incomplete inside the machine’s domain.
	\parencite{filimowicz2025}
\end{quote}


\section{UNDISTRIBUTED/TBD}

\begin{orangebox}
	Experimental parts
\end{orangebox}


\subsection{Creativity: Discrete vs. Continuous}

\begin{orangebox}
	What is Deleuze and Guattari’s assumption about desire? Is desire the initiation of creativity? Is the schizoprocess a release of essential human potential?

	The association of desire with creativity is unmistakably present throughout the work of \gls{dg}. However, neither desire nor the schizoprocess should be mistaken as mere catalysts that unleash an otherwise dormant human creativity. The schizoprocess is not a secondary mechanism; it is the form of desire itself in motion. Schizz, as \gls{dg} term it, is not an event that activates creativity, but the name for the production of desire as such. It is the nature and source of desiring-production. In this framework, creativity is not a supplement to desire,it is its immanent operation.

	Human consciousness is not a site of passive receptivity but is itself generative. It is productive of production, productive of desiring-production. Desire's primary function, then, is not expression, not representation, but production: the production of production. It is defined not by lack, but by abundance \parencite[49]{buchanan2008e}. Desiring-production is fundamentally machinic,it binds together partial objects that are by nature \textit{fragmentary and fragmented}. Desire is thus the coupling of flows and interruptions, a dynamic interplay of continuous intensities and discrete interruptions \parencite[5]{deleuze1983}.

	Rather than envisioning creativity as a discrete act,a spark or insight emerging from nowhere,\gls{dg} posit a continuous field of creative productivity, where breaks and ruptures are part of the process itself. The schizz is not a deviation from order; it is the generative logic of how meaning and subjectivity emerge. This distinction,between the discrete and the continuous,is vital not only to understanding desire and creativity, but also to the broader analysis of control societies and algorithmic governance pursued throughout this thesis.
\end{orangebox}



\subsection{Dividuation}
\begin{quote}
	What is a dividual? A dividual is a bundle of elements held together in variation  rather than in reference to a unitary subject. Where disciplinary institutions  segmented the life-course of individuals into separate subjective roles and  functions, control modulates elements of subjectivity across the entire social field.
	\parencite[5]{mackenzie2021}
\end{quote}


\section{Difference, Repetition, Singularity (Potential discussion about the
  need for sensory input for the genAI (LeCUn?)}
The role of the imagination, or the mind which contemplates in its multiple and fragmented states, is to draw something new from repetition, to draw difference from it. For that matter, repetition is itself in essence imaginary, since the imagination alone here forms the “moment” of the vis repetitiva from the point of view of constitution: it makes that which it contracts appear as elements or cases of repetition. Imaginary repetition is not a false repetition which stands in for the absent true repetition: true repetition takes place in imagination. \cite{deleuze1994, kruger2021}

Once manifested as thought, furthermore, the thinking that happens is divergent and ramifying rather than convergent and identifying. \cite[175]{kruger2021}

Thought emerges out of an evanescent materiality. It is exactly at this point where Deleuze parts ways with Kant. While the latter accepted the existence of a priori categories of mind that would stabilise and universalise the thought of the thinking subject, Deleuze maintains the radically empirical nature of the emergence of any transcendental structures. Thought emerges out of experience and can only ever be a response to experience. Experience, in turn, is bound up with matter, in the non-identical repetition of material intensities. \cite[178]{kruger2021}


\section{Assumption of indifference between the institutions of control}

As \cite[13]{mackenzie2021} points out, Deleuze's formalisation of the
institutions of control is making emphasis on the unification of the
institutional framework throug the technological means. Howeveras the borders
of agency is getting blurred, the differences in agency of the \gls{ai} models
is making a huge difference.
