# Author N/A - Latzer Just (2020)

## Highlights and notes

- provided by Gillespie: “encoded procedures for transforming input data into a desired output, based on specified calculations” (Gillespie, 2014, p.167)

  highlight @ page 2-2

- communications, discussions about the governance by algorithms often focus on the influence on opinion formation and consequently the potential for its manipulation during polls and elections by way of personalization

  highlight @ page 5-5

- which is executed via biased filter bubbles (decreasing variety of content), microtargeting (e.g. , political microtargeting, see Zuiderveen Borgesius et al. , 2018; or online behavioral advertising, see Boerman, Kruikemeier, & Zuiderveen Borgesius, 2017), social bots (Ferrara, Varol, Davis, Menczer, & Flammini, 2016), and automatically produced content (algorithmic journalism, see Dörr, 2016). These examples are predominantly discussed as implications of algorithms for the traditional news/media sector

  highlight @ page 5-5

- Altogether, algorithmic governance and its implications have received the most attention in research on social and political orientation. Search applications, content scoring, and news aggregators are understood as intermediaries (Bui, 2010; Newman, Fletcher, Kalogeropoulos, Levy, & Nielsen, 2018) between traditional mass media and individual news consumption. Empirical research suggests that algorithmic selection applications will become more important for information retrieval in the future (Newman et al. , 2018; Shearer & Matsa, 2018). This is accompanied by fears of misinformation online (Lazer et al. , 2018; Vosoughi, Roy, & Aral, 2018), where deliberately disseminated false news is perceived as a threat to opinion formation, for example in the context of elections (Allcott & Gentzkow, 2017), or by fears of echo chambers (Sunstein, 2001) or personalized filter bubbles (Pariser, 2012), leading to fragmented, biased perceptions of society (Dylko, 2016). Empirical studies are inconclusive: there is evidence of clear patterns of algorithmically induced, homogeneous opinion networks (Bakshy, Messing, & Adamic, 2015; Dylko et al. , 2018), but other studies indicate more opinion diversity despite algorithmic selection, and therefore give empirical evidence of a lower risk of echo chambers and filter bubbles (Barberá, Jost, Nagler, Tucker, & Bonneau, 2015; Dubois & Blank, 2018; Fletcher & Nielsen, 2017; Möller, Trilling, Helberger, & Es, 2018; Zuiderveen Borgesius et al. , 2016)

  highlight @ page 5-5

- Overall, because of the wide scope and diffusion of algorithmic Internet applications, the effects of governance by algorithms can be framed as shaping and influencing individuals’ reality construction in everyday life (Berger & Luckmann, 1967

  highlight @ page 6-6

- This algorithmically co-shaped reality construction (Just & Latzer, 2017) differs widely from the conventional reality construction by mass media (Luhmann, 1996), in particular regarding personalization and the constellation of the actors involved

  highlight @ page 6-6

- Within the broad body of literature on the governance of algorithms, different approaches that somehow tackle the same concerns and problems about algorithmic selections can be identified, although in various constellations and to differing extents. Despite the lack of precise delineations, four approaches can be distinguished. The first three are (1) riskbased approaches, (2) human-rights-based approaches, and (3) ethics-based approaches. In addition, a myriad of general principles (as opposed to specific or detailed rules) are commonly highlighted throughout this literature. Among these are principles such as fairness, transparency, accountability, liability, and explainability, which should be taken into consideration in the governance of algorithms. This strand of literature could be grouped as (4) a fourth category, as four principles-based approaches.

  highlight @ page 7-7

