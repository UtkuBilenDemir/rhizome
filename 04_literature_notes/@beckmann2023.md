---
category: literature_note
aliases: 
  - An Alternative to Cognitivism: Computational Phenomenology for Deep Learning
  - beckmann2023
tags:
  - literature_note
  - zotero
  - üéØ
citekey: beckmann2023
---

|       Created       |    Last Modified    |          Exists Since           |
| :-----------------: | :-----------------: | :-----------------------------: |
| `= this.file.ctime` | `= this.file.mtime` | `= date(now) - this.file.ctime` |
>[!info] Metadata
> **FirstAuthor**:: Beckmann, Pierre  
> **Author**:: K√∂stner, Guillaume  
> **Author**:: Hip√≥lito, In√™s  
~    
> **Title**:: An Alternative to Cognitivism: Computational Phenomenology for Deep Learning  
> **Year**:: 2023   
> **Citekey**:: beckmann2023  
> **itemType**:: journalArticle  
> **Journal**:: *Minds and Machines*  
> **Volume**:: 33  
> **Issue**:: 3   
> **Pages**:: 397-427  
> **url**:: https://link.springer.com/10.1007/s11023-023-09638-w
> **DOI**:: 10.1007/s11023-023-09638-w    

> [!link]-
> zotero_link:: [Beckmann et al._2023_An Alternative to Cognitivism Computational Phenomenology for Deep Learning.pdf](zotero://select/library/items/JV426ZLF)

> [!cite]-
> citekey:: beckmann2023

> [!abstract]-
> abstract:: Abstract
            We propose a non-representationalist framework for deep learning relying on a novel method computational phenomenology, a dialogue between the first-person perspective (relying on phenomenology) and the mechanisms of computational models. We thereby propose an alternative to the modern cognitivist interpretation of deep learning, according to which artificial neural networks encode representations of external entities. This interpretation mainly relies on neuro-representationalism, a position that combines a strong ontological commitment towards scientific theoretical entities and the idea that the brain operates on symbolic representations of these entities. We proceed as follows: after offering a review of cognitivism and neuro-representationalism in the field of deep learning, we first elaborate a phenomenological critique of these positions; we then sketch out computational phenomenology and distinguish it from existing alternatives; finally we apply this new method to deep learning models trained on specific tasks, in order to formulate a conceptual framework of deep-learning, that allows one to think of artificial neural networks‚Äô mechanisms in terms of lived experience.

> [!keywords]-
> keywords:: üéØ

> [!authors]-
> authors:: Pierre Beckmann, Guillaume K√∂stner, In√™s Hip√≥lito

> [!related]-

```dataview
TABLE created, updated as modified, tags, type
FROM ""
WHERE related != null
AND contains(related, "beckmann2023")
```

> [!hypothesis]-
> hypothesis:: 

> [!methodology]- 
> methodology:: 

> [!result]- Result(s) 
> results::

> [!summary]- Summary of Key Points
> summary:: 

# Notes

| <mark class="hltr-grey">Highlight Color</mark> | Meaning                       |
| ---------------------------------------------- | ----------------------------- |
| <mark class="hltr-yellow">Yellow</mark>        | Interesting Point             |
| <mark class="hltr-orange">Orange</mark>        | Important Point By Author     |
| <mark class="hltr-red">Red</mark>              | Disagree with Author          |
| <mark class="hltr-blue">Blue</mark>            | Support the Author            |
| <mark class="hltr-magenta">Magenta</mark>      | Important To Me               |
| <mark class="hltr-purple">Purple</mark>        | Literary Note To Lookup Later |
| <mark class="hltr-green">Green</mark>          | Example                       |
| <mark class="hltr-grey">Grey</mark>            | New term, - definition        |

- <mark class="hltr-yellow">"More often than not, hidden layers have fewer neurons than the input layer to force the network to learn compressed representations of the original input. For example, while our eyes obtain raw pixel values from our surroundings, our brain thinks in terms of edges and contours. This is because the hidden layers of biological neurons in our brain force us to come up with better representations for everything we perceive. (Buduma et al., 2022)‚Äù</mark> [Page 3](zotero://open-pdf/library/items/JV426ZLF?page=3&annotation=XWDRE23Z) 
	- <mark class="hltr-orange">"world models‚Äù</mark> [Page 3](zotero://open-pdf/library/items/JV426ZLF?page=3&annotation=J8R5CK6D) 

> [!context]-
> ==(How this article relates to other work in the field; how it ties in with key issues and findings by others, including yourself)==
> context:: 

> [!significance]-
> ==(to the field; in relation to your own work)==
> significance:: 